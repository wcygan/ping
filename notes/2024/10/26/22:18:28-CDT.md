# 10:18PM Update

What direction are you trying to head in?

Currently, the focus is about bootstrapping `ping-processor` and running it on Kubernetes.

Maybe you can scale the replica count of everything down to 1 while it's under active development. You already know that replica count 3 works, but it takes a bit longer to bootstrap.

I'm not quite sure what's going on with `ping-processor` right now and why it doesn't have a pod spinning up...

Here's what I know:

```bash
kubectl get flinkdeployment ping-processor
NAME             JOB STATUS   LIFECYCLE STATE
ping-processor                FAILED

kubectl describe flinkdeployment ping-processor
Name:         ping-processor
Namespace:    default
Labels:       <none>
Annotations:  <none>
API Version:  flink.apache.org/v1beta1
Kind:         FlinkDeployment
Metadata:
  Creation Timestamp:  2024-10-27T03:17:46Z
  Finalizers:
    flinkdeployments.flink.apache.org/finalizer
  Generation:        2
  Resource Version:  2740
  UID:               82373250-e654-494f-9407-16cd2c102f9d
Spec:
  Flink Configuration:
    execution.checkpointing.externalized-checkpoint-retention:  RETAIN_ON_CANCELLATION
    execution.checkpointing.interval:                           10s
    execution.checkpointing.min-pause:                          5s
    execution.checkpointing.mode:                               EXACTLY_ONCE
    execution.checkpointing.timeout:                            10m
    metrics.reporter.prometheus.class:                          org.apache.flink.metrics.prometheus.PrometheusReporter
    metrics.reporter.prometheus.port:                           9249
    metrics.reporters:                                          prometheus
    state.checkpoints.dir:                                      file:///opt/flink/data/checkpoints
    state.savepoints.dir:                                       file:///opt/flink/data/savepoints
    taskmanager.numberOfTaskSlots:                              2
  Flink Version:                                                v1_17
  Image:                                                        ping-processor
  Job:
    Args:
    Entry Class:   com.ping.PingStreamProcessor
    Jar URI:       local:///opt/flink/usrlib/ping-processor.jar
    Parallelism:   2
    State:         running
    Upgrade Mode:  last-state
  Job Manager:
    Replicas:  1
    Resource:
      Cpu:          1
      Memory:       2048
  Service Account:  flink
  Task Manager:
    Resource:
      Cpu:     1
      Memory:  2048
Status:
  Cluster Info:
  Error:                          {"type":"org.apache.flink.kubernetes.operator.exception.ValidationException","message":"Job could not be upgraded with last-state while HA disabled","additionalMetadata":{},"throwableList":[]}
  Job Manager Deployment Status:  MISSING
  Job Status:
    Checkpoint Info:
      Last Periodic Checkpoint Timestamp:  0
    Savepoint Info:
      Last Periodic Savepoint Timestamp:  0
      Savepoint History:
  Lifecycle State:  FAILED
  Reconciliation Status:
    Reconciliation Timestamp:  0
    State:                     UPGRADING
Events:
  Type     Reason           Age    From      Message
  ----     ------           ----   ----      -------
  Warning  ValidationError  6m51s  Operator  Job could not be upgraded with last-state while HA disabled
```

## Exactly Once Processing

I think it's important to use exactly once processing for the `ping-processor` job. This is because we don't want to lose any data or process the same data multiple times.

Beyond that, I don't care too much about the configuration of the job. I just want it to work.

```bash
kubectl get flinkdeployment ping-processor
kubectl describe flinkdeployment ping-processor
```
