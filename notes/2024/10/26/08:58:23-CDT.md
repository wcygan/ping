# Seeing the data being replicated from primary to replica

```bash
wcygan@foobar ping % curl -X POST http://localhost:8080/ping.v1.PingService/Ping \
     -H "Content-Type: application/json" \
     -d '{"timestamp_ms": 1728926331000}'
{}
                                  
wcygan@foobar ping % kubectl cnpg psql pg-cluster --replica -- -d pingdb -c "SELECT * FROM pings;"
                  id                  |       pinged_at        
--------------------------------------+------------------------
 63b44e6e-84e7-43c3-8410-57afb4d567df | 2024-10-14 17:18:51+00
 da400417-7b22-415f-a960-15549d44211c | 2024-10-14 17:18:51+00
 1c1b657f-76a1-49e4-a0b1-e17da7f4bdef | 2024-10-14 17:18:51+00
 a0d14951-f20f-4fe7-be96-fbae97333845 | 2024-10-14 17:18:51+00
(4 rows)

wcygan@foobar ping % kubectl cnpg psql pg-cluster -- -d pingdb -c "SELECT * FROM pings;"          
                  id                  |       pinged_at        
--------------------------------------+------------------------
 63b44e6e-84e7-43c3-8410-57afb4d567df | 2024-10-14 17:18:51+00
 da400417-7b22-415f-a960-15549d44211c | 2024-10-14 17:18:51+00
 1c1b657f-76a1-49e4-a0b1-e17da7f4bdef | 2024-10-14 17:18:51+00
 a0d14951-f20f-4fe7-be96-fbae97333845 | 2024-10-14 17:18:51+00
(4 rows)
```

We can see here that somedata is being replicated from the primary to the replica. The data is the same on both the primary and the replica.

## Replication Logs

- **WAL Streaming Status:**
  - Look for messages like "started streaming WAL from primary at X/X on timeline X"
  - Example from logs: "started streaming WAL from primary at 0/4000000 on timeline 1"

- **Recovery and Replay Progress:**
  - Messages showing LSN (Log Sequence Number) positions
  - Example: "recovery restart point at 0/A0002A0"
  - Shows both write_lsn and replay_lsn positions

- **Checkpoint/Restartpoint Information:**
  - Shows how much data is being processed
  - Example: "restartpoint complete: wrote 3 buffers (0.0%)"
  - Includes details about WAL files added/removed

- **Replication Health:**
  - Watch for FATAL messages that might indicate replication issues
  - Example: "terminating walreceiver due to timeout"

You can view these logs anytime using:

```bash
kubectl logs pg-cluster-2 -c postgres
```

### Examples

You can see some information regarding this directly in the logs of the replica pod:

```bash
kubectl logs pg-cluster-2 -c postgres | rg 'WAL'
{"level":"info","ts":"2024-10-26T04:41:26.327805259Z","msg":"Checking for free disk space for WALs before starting PostgreSQL","logger":"instance-manager","logging_pod":"pg-cluster-2"}
{"level":"info","ts":"2024-10-26T04:41:26.470221916Z","logger":"pg_controldata","msg":"pg_control version number:            1700\nCatalog version number:               202406281\nDatabase system identifier:           7429939782732197911\nDatabase cluster state:               in production\npg_control last modified:             Sat 26 Oct 2024 04:41:21 AM UTC\nLatest checkpoint location:           0/3000080\nLatest checkpoint's REDO location:    0/3000028\nLatest checkpoint's REDO WAL file:    000000010000000000000003\nLatest checkpoint's TimeLineID:       1\nLatest checkpoint's PrevTimeLineID:   1\nLatest checkpoint's full_page_writes: on\nLatest checkpoint's NextXID:          0:743\nLatest checkpoint's NextOID:          24578\nLatest checkpoint's NextMultiXactId:  1\nLatest checkpoint's NextMultiOffset:  0\nLatest checkpoint's oldestXID:        730\nLatest checkpoint's oldestXID's DB:   1\nLatest checkpoint's oldestActiveXID:  743\nLatest checkpoint's oldestMultiXid:   1\nLatest checkpoint's oldestMulti's DB: 1\nLatest checkpoint's oldestCommitTsXid:0\nLatest checkpoint's newestCommitTsXid:0\nTime of latest checkpoint:            Sat 26 Oct 2024 04:41:19 AM UTC\nFake LSN counter for unlogged rels:   0/3E8\nMinimum recovery ending location:     0/0\nMin recovery ending loc's timeline:   0\nBackup start location:                0/0\nBackup end location:                  0/0\nEnd-of-backup record required:        no\nwal_level setting:                    logical\nwal_log_hints setting:                on\nmax_connections setting:              100\nmax_worker_processes setting:         32\nmax_wal_senders setting:              10\nmax_prepared_xacts setting:           0\nmax_locks_per_xact setting:           64\ntrack_commit_timestamp setting:       off\nMaximum data alignment:               8\nDatabase block size:                  8192\nBlocks per segment of large relation: 131072\nWAL block size:                       8192\nBytes per WAL segment:                16777216\nMaximum length of identifiers:        64\nMaximum columns in an index:          32\nMaximum size of a TOAST chunk:        1996\nSize of a large-object chunk:         2048\nDate/time type storage:               64-bit integers\nFloat8 argument passing:              by value\nData page checksum version:           0\nMock authentication nonce:            a34d019f85073b4ad04880fa6899937f6eedc5b34f254fdf92600a575ad9dc6b\n","pipe":"stdout","logging_pod":"pg-cluster-2"}
{"level":"info","ts":"2024-10-26T04:41:26.997614723Z","logger":"postgres","msg":"record","logging_pod":"pg-cluster-2","record":{"log_time":"2024-10-26 04:41:26.997 UTC","process_id":"69","session_id":"671c72f6.45","session_line_num":"1","session_start_time":"2024-10-26 04:41:26 UTC","transaction_id":"0","error_severity":"LOG","sql_state_code":"00000","message":"started streaming WAL from primary at 0/4000000 on timeline 1","backend_type":"walreceiver","query_id":"0"}}
{"level":"info","ts":"2024-10-26T04:46:36.219525956Z","logger":"postgres","msg":"record","logging_pod":"pg-cluster-2","record":{"log_time":"2024-10-26 04:46:36.219 UTC","process_id":"28","session_id":"671c72f6.1c","session_line_num":"2","session_start_time":"2024-10-26 04:41:26 UTC","transaction_id":"0","error_severity":"LOG","sql_state_code":"00000","message":"restartpoint complete: wrote 97 buffers (0.6%); 0 WAL file(s) added, 0 removed, 0 recycled; write=9.623 s, sync=0.053 s, total=9.703 s; sync files=51, longest=0.010 s, average=0.002 s; distance=32768 kB, estimate=32768 kB; lsn=0/5000080, redo lsn=0/5000028","backend_type":"checkpointer","query_id":"0"}}
{"level":"info","ts":"2024-10-26T04:51:26.507135081Z","logger":"postgres","msg":"record","logging_pod":"pg-cluster-2","record":{"log_time":"2024-10-26 04:51:26.506 UTC","process_id":"28","session_id":"671c72f6.1c","session_line_num":"5","session_start_time":"2024-10-26 04:41:26 UTC","transaction_id":"0","error_severity":"LOG","sql_state_code":"00000","message":"restartpoint complete: wrote 3 buffers (0.0%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.209 s, sync=0.008 s, total=0.239 s; sync files=3, longest=0.004 s, average=0.003 s; distance=16902 kB, estimate=31181 kB; lsn=0/7000098, redo lsn=0/60818E8","backend_type":"checkpointer","query_id":"0"}}
{"level":"info","ts":"2024-10-26T04:56:26.696013284Z","logger":"postgres","msg":"record","logging_pod":"pg-cluster-2","record":{"log_time":"2024-10-26 04:56:26.695 UTC","process_id":"28","session_id":"671c72f6.1c","session_line_num":"8","session_start_time":"2024-10-26 04:41:26 UTC","transaction_id":"0","error_severity":"LOG","sql_state_code":"00000","message":"restartpoint complete: wrote 1 buffers (0.0%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.106 s, sync=0.003 s, total=0.144 s; sync files=1, longest=0.003 s, average=0.003 s; distance=15866 kB, estimate=29649 kB; lsn=0/8000060, redo lsn=0/70003E8","backend_type":"checkpointer","query_id":"0"}}
{"level":"info","ts":"2024-10-26T05:01:26.735483837Z","logger":"postgres","msg":"record","logging_pod":"pg-cluster-2","record":{"log_time":"2024-10-26 05:01:26.735 UTC","process_id":"28","session_id":"671c72f6.1c","session_line_num":"11","session_start_time":"2024-10-26 04:41:26 UTC","transaction_id":"0","error_severity":"LOG","sql_state_code":"00000","message":"restartpoint complete: wrote 0 buffers (0.0%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.001 s, sync=0.001 s, total=0.036 s; sync files=0, longest=0.000 s, average=0.000 s; distance=16387 kB, estimate=28323 kB; lsn=0/9000060, redo lsn=0/80010D0","backend_type":"checkpointer","query_id":"0"}}
{"level":"info","ts":"2024-10-26T05:08:17.679772479Z","logger":"postgres","msg":"record","logging_pod":"pg-cluster-2","record":{"log_time":"2024-10-26 05:08:17.677 UTC","process_id":"69","session_id":"671c72f6.45","session_line_num":"2","session_start_time":"2024-10-26 04:41:26 UTC","transaction_id":"0","error_severity":"FATAL","sql_state_code":"08006","message":"could not receive data from WAL stream: SSL connection has been closed unexpectedly","backend_type":"walreceiver","query_id":"0"}}
```


You may also be able to tail the logs:

```bash
kubectl logs -f pg-cluster-2 -c postgres
```