# Kafka in Kraft mode

<https://strimzi.io/docs/operators/latest/deploying#assembly-kraft-mode-str>

KRaft (Kafka Raft metadata) mode replaces Kafka’s dependency on ZooKeeper for cluster management. KRaft mode simplifies the deployment and management of Kafka clusters by bringing metadata management and coordination of clusters into Kafka.

Kafka in KRaft mode is designed to offer enhanced reliability, scalability, and throughput. Metadata operations become more efficient as they are directly integrated. And by removing the need to maintain a ZooKeeper cluster, there’s also a reduction in the operational and security overhead.

To deploy a Kafka cluster in KRaft mode, you must use Kafka and KafkaNodePool custom resources. The Kafka resource using KRaft mode must also have the annotations strimzi.io/kraft: enabled and strimzi.io/node-pools: enabled. For more details and examples, see Deploying a Kafka cluster in KRaft mode.

Through node pool configuration using KafkaNodePool resources, nodes are assigned the role of broker, controller, or both:

Controller nodes operate in the control plane to manage cluster metadata and the state of the cluster using a Raft-based consensus protocol.

Broker nodes operate in the data plane to manage the streaming of messages, receiving and storing data in topic partitions.

Dual-role nodes fulfill the responsibilities of controllers and brokers.

Controllers use a metadata log, stored as a single-partition topic (__cluster_metadata) on every node, which records the state of the cluster. When requests are made to change the cluster configuration, an active (lead) controller manages updates to the metadata log, and follower controllers replicate these updates. The metadata log stores information on brokers, replicas, topics, and partitions, including the state of in-sync replicas and partition leadership. Kafka uses this metadata to coordinate changes and manage the cluster effectively.

Broker nodes act as observers, storing the metadata log passively to stay up-to-date with the cluster’s state. Each node fetches updates to the log independently.

Note
The KRaft metadata version used in the Kafka cluster must be supported by the Kafka version in use. Both versions are managed through the Kafka resource configuration. For more information, see Configuring Kafka in KRaft mode.
In the following example, a Kafka cluster comprises a quorum of controller and broker nodes for fault tolerance and high availability.

If you are using ZooKeeper for metadata management in your Kafka cluster, you can migrate to using Kafka in KRaft mode.

During the migration, you install a quorum of controller nodes as a node pool, which replaces ZooKeeper for management of your cluster. You enable KRaft migration in the cluster configuration by applying the strimzi.io/kraft="migration" annotation. After the migration is complete, you switch the brokers to using KRaft and the controllers out of migration mode using the strimzi.io/kraft="enabled" annotation.

Before starting the migration, verify that your environment can support Kafka in KRaft mode, as there are a number of limitations. Note also, the following:

Migration is only supported on dedicated controller nodes, not on nodes with dual roles as brokers and controllers.

Throughout the migration process, ZooKeeper and controller nodes operate in parallel for a period, requiring sufficient compute resources in the cluster.

Once KRaft mode is enabled, rollback to ZooKeeper is not possible. Consider this carefully before proceeding with the migration.

Prerequisites
You must be using Strimzi 0.40 or newer with Kafka 3.7.0 or newer. If you are using an earlier version of Strimzi or Apache Kafka, upgrade before migrating to KRaft mode.

Verify that the ZooKeeper-based deployment is operating without the following, as they are not supported in KRaft mode:

JBOD storage. While the jbod storage type can be used, the JBOD array must contain only one disk.

The Cluster Operator that manages the Kafka cluster is running.

The Kafka cluster deployment uses Kafka node pools.

If your ZooKeeper-based cluster is already using node pools, it is ready to migrate. If not, you can migrate the cluster to use node pools. To migrate when the cluster is not using node pools, brokers must be contained in a KafkaNodePool resource configuration that is assigned a broker role and has the name kafka. Support for node pools is enabled in the Kafka resource configuration using the strimzi.io/node-pools: enabled annotation.

Important
Using a single controller with ephemeral storage for migrating to KRaft will not work. During the migration, controller restart will cause loss of metadata synced from ZooKeeper (such as topics and ACLs). In general, migrating an ephemeral-based ZooKeeper cluster to KRaft is not recommended.
In this procedure, the Kafka cluster name is my-cluster, which is located in the my-project namespace. The name of the controller node pool created is controller. The node pool for the brokers is called kafka.

Procedure
For the Kafka cluster, create a node pool with a controller role.

The node pool adds a quorum of controller nodes to the cluster.

Example configuration for a controller node pool
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaNodePool
metadata:
  name: controller
  labels:
    strimzi.io/cluster: my-cluster
spec:
  replicas: 3
  roles:
    - controller
  storage:
    type: jbod
    volumes:
      - id: 0
        type: persistent-claim
        size: 20Gi
        deleteClaim: false
    resources:
      requests:
        memory: 64Gi
        cpu: "8"
      limits:
        memory: 64Gi
        cpu: "12"
Note
For the migration, you cannot use a node pool of nodes that share the broker and controller roles.
Apply the new KafkaNodePool resource to create the controllers.

Errors related to using controllers in a ZooKeeper-based environment are expected in the Cluster Operator logs. The errors can block reconciliation. To prevent this, perform the next step immediately.

Enable KRaft migration in the Kafka resource by setting the strimzi.io/kraft annotation to migration:

kubectl annotate kafka my-cluster strimzi.io/kraft="migration" --overwrite
Enabling KRaft migration
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: my-cluster
  namespace: my-project
  annotations:
    strimzi.io/kraft: migration
# ...
Applying the annotation to the Kafka resource configuration starts the migration.

Check the controllers have started and the brokers have rolled:

kubectl get pods -n my-project
Output shows nodes in broker and controller node pools
NAME                     READY  STATUS   RESTARTS
my-cluster-kafka-0       1/1    Running  0
my-cluster-kafka-1       1/1    Running  0
my-cluster-kafka-2       1/1    Running  0
my-cluster-controller-3  1/1    Running  0
my-cluster-controller-4  1/1    Running  0
my-cluster-controller-5  1/1    Running  0
# ...
Check the status of the migration:

kubectl get kafka my-cluster -n my-project -w
Updates to the metadata state
NAME        ...  METADATA STATE
my-cluster  ...  Zookeeper
my-cluster  ...  KRaftMigration
my-cluster  ...  KRaftDualWriting
my-cluster  ...  KRaftPostMigration
METADATA STATE shows the mechanism used to manage Kafka metadata and coordinate operations. At the start of the migration this is ZooKeeper.

ZooKeeper is the initial state when metadata is only stored in ZooKeeper.

KRaftMigration is the state when the migration is in progress. The flag to enable ZooKeeper to KRaft migration (zookeeper.metadata.migration.enable) is added to the brokers and they are rolled to register with the controllers. The migration can take some time at this point depending on the number of topics and partitions in the cluster.

KRaftDualWriting is the state when the Kafka cluster is working as a KRaft cluster, but metadata are being stored in both Kafka and ZooKeeper. Brokers are rolled a second time to remove the flag to enable migration.

KRaftPostMigration is the state when KRaft mode is enabled for brokers. Metadata are still being stored in both Kafka and ZooKeeper.

The migration status is also represented in the status.kafkaMetadataState property of the Kafka resource.

Warning
You can roll back to using ZooKeeper from this point. The next step is to enable KRaft. Rollback cannot be performed after enabling KRaft.
When the metadata state has reached KRaftPostMigration, enable KRaft in the Kafka resource configuration by setting the strimzi.io/kraft annotation to enabled:

kubectl annotate kafka my-cluster strimzi.io/kraft="enabled" --overwrite
Enabling KRaft migration
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: my-cluster
  namespace: my-project
  annotations:
    strimzi.io/kraft: enabled
# ...
Check the status of the move to full KRaft mode:

kubectl get kafka my-cluster -n my-project -w
Updates to the metadata state
NAME        ...  METADATA STATE
my-cluster  ...  Zookeeper
my-cluster  ...  KRaftMigration
my-cluster  ...  KRaftDualWriting
my-cluster  ...  KRaftPostMigration
my-cluster  ...  PreKRaft
my-cluster  ...  KRaft
PreKRaft is the state when all ZooKeeper-related resources have been automatically deleted.

KRaft is the final state (after the controllers have rolled) when the KRaft migration is finalized.

Note
Depending on how deleteClaim is configured for ZooKeeper, its Persistent Volume Claims (PVCs) and persistent volumes (PVs) may not be deleted. deleteClaim specifies whether the PVC is deleted when the cluster is uninstalled. The default is false.
Remove any ZooKeeper-related configuration from the Kafka resource.

Remove the following section:

spec.zookeeper

If present, you can also remove the following options from the .spec.kafka.config section:

log.message.format.version

inter.broker.protocol.version

Removing log.message.format.version and inter.broker.protocol.version causes the brokers and controllers to roll again. Removing ZooKeeper properties removes any warning messages related to ZooKeeper configuration being present in a KRaft-operated cluster.

## Deployment

6. Deploying Strimzi using installation artifacts
Having prepared your environment for a deployment of Strimzi, you can deploy Strimzi to a Kubernetes cluster. Use the installation files provided with the release artifacts.

You can deploy Strimzi 0.43.0 on Kubernetes 1.23 and later.

The steps to deploy Strimzi using the installation files are as follows:

Deploy the Cluster Operator

Use the Cluster Operator to deploy the following:

Kafka cluster

Topic Operator

User Operator

Optionally, deploy the following Kafka components according to your requirements:

Kafka Connect

Kafka MirrorMaker

Kafka Bridge

Note
To run the commands in this guide, a Kubernetes user must have the rights to manage role-based access control (RBAC) and CRDs.
6.1. Basic deployment path
You can set up a deployment where Strimzi manages a single Kafka cluster in the same namespace. You might use this configuration for development or testing. Or you can use Strimzi in a production environment to manage a number of Kafka clusters in different namespaces.

The basic deployment path is as follows:

Download the release artifacts

Create a Kubernetes namespace in which to deploy the Cluster Operator

Deploy the Cluster Operator

Update the install/cluster-operator files to use the namespace created for the Cluster Operator

Install the Cluster Operator to watch one, multiple, or all namespaces

Create a Kafka cluster

After which, you can deploy other Kafka components and set up monitoring of your deployment.

6.2. Deploying the Cluster Operator
The first step for any deployment of Strimzi is to install the Cluster Operator, which is responsible for deploying and managing Kafka clusters within a Kubernetes cluster. A single command applies all the installation files in the install/cluster-operator folder: kubectl apply -f ./install/cluster-operator.

The command sets up everything you need to be able to create and manage a Kafka deployment, including the following resources:

Cluster Operator (Deployment, ConfigMap)

Strimzi CRDs (CustomResourceDefinition)

RBAC resources (ClusterRole, ClusterRoleBinding, RoleBinding)

Service account (ServiceAccount)

Cluster-scoped resources like CustomResourceDefinition, ClusterRole, and ClusterRoleBinding require administrator privileges for installation. Prior to installation, it’s advisable to review the ClusterRole specifications to ensure they do not grant unnecessary privileges.

After installation, the Cluster Operator runs as a regular Deployment to watch for updates of Kafka resources. Any standard (non-admin) Kubernetes user with privileges to access the Deployment can configure it. A cluster administrator can also grant standard users the privileges necessary to manage Strimzi custom resources.

By default, a single replica of the Cluster Operator is deployed. You can add replicas with leader election so that additional Cluster Operators are on standby in case of disruption. For more information, see Running multiple Cluster Operator replicas with leader election.

6.2.1. Specifying the namespaces the Cluster Operator watches
The Cluster Operator watches for updates in the namespaces where the Kafka resources are deployed. When you deploy the Cluster Operator, you specify which namespaces to watch in the Kubernetes cluster. You can specify the following namespaces:

A single selected namespace (the same namespace containing the Cluster Operator)

Multiple selected namespaces

All namespaces in the cluster

Watching multiple selected namespaces has the most impact on performance due to increased processing overhead. To optimize performance for namespace monitoring, it is generally recommended to either watch a single namespace or monitor the entire cluster. Watching a single namespace allows for focused monitoring of namespace-specific resources, while monitoring all namespaces provides a comprehensive view of the cluster’s resources across all namespaces.

The Cluster Operator watches for changes to the following resources:

Kafka for the Kafka cluster.

KafkaConnect for the Kafka Connect cluster.

KafkaConnector for creating and managing connectors in a Kafka Connect cluster.

KafkaMirrorMaker for the Kafka MirrorMaker instance.

KafkaMirrorMaker2 for the Kafka MirrorMaker 2 instance.

KafkaBridge for the Kafka Bridge instance.

KafkaRebalance for the Cruise Control optimization requests.

When one of these resources is created in the Kubernetes cluster, the operator gets the cluster description from the resource and starts creating a new cluster for the resource by creating the necessary Kubernetes resources, such as Deployments, Pods, Services and ConfigMaps.

Each time a Kafka resource is updated, the operator performs corresponding updates on the Kubernetes resources that make up the cluster for the resource.

Resources are either patched or deleted, and then recreated in order to make the cluster for the resource reflect the desired state of the cluster. This operation might cause a rolling update that might lead to service disruption.

When a resource is deleted, the operator undeploys the cluster and deletes all related Kubernetes resources.

Note
While the Cluster Operator can watch one, multiple, or all namespaces in a Kubernetes cluster, the Topic Operator and User Operator watch for KafkaTopic and KafkaUser resources in a single namespace. For more information, see Watching Strimzi resources in Kubernetes namespaces.
6.2.2. Deploying the Cluster Operator to watch a single namespace
This procedure shows how to deploy the Cluster Operator to watch Strimzi resources in a single namespace in your Kubernetes cluster.

Prerequisites
You need an account with permission to create and manage CustomResourceDefinition and RBAC (ClusterRole, and RoleBinding) resources.

Procedure
Edit the Strimzi installation files to use the namespace the Cluster Operator is going to be installed into.

For example, in this procedure the Cluster Operator is installed into the namespace my-cluster-operator-namespace.

On Linux, use:

sed -i 's/namespace: .*/namespace: my-cluster-operator-namespace/' install/cluster-operator/*RoleBinding*.yaml
On MacOS, use:

sed -i '' 's/namespace: .*/namespace: my-cluster-operator-namespace/' install/cluster-operator/*RoleBinding*.yaml
Deploy the Cluster Operator:

kubectl create -f install/cluster-operator -n my-cluster-operator-namespace
Check the status of the deployment:

kubectl get deployments -n my-cluster-operator-namespace
Output shows the deployment name and readiness
NAME                      READY  UP-TO-DATE  AVAILABLE
strimzi-cluster-operator  1/1    1           1
READY shows the number of replicas that are ready/expected. The deployment is successful when the AVAILABLE output shows 1.

6.2.3. Deploying the Cluster Operator to watch multiple namespaces
This procedure shows how to deploy the Cluster Operator to watch Strimzi resources across multiple namespaces in your Kubernetes cluster.

Prerequisites
You need an account with permission to create and manage CustomResourceDefinition and RBAC (ClusterRole, and RoleBinding) resources.

Procedure
Edit the Strimzi installation files to use the namespace the Cluster Operator is going to be installed into.

For example, in this procedure the Cluster Operator is installed into the namespace my-cluster-operator-namespace.

On Linux, use:

sed -i 's/namespace: .*/namespace: my-cluster-operator-namespace/' install/cluster-operator/*RoleBinding*.yaml
On MacOS, use:

sed -i '' 's/namespace: .*/namespace: my-cluster-operator-namespace/' install/cluster-operator/*RoleBinding*.yaml
Edit the install/cluster-operator/060-Deployment-strimzi-cluster-operator.yaml file to add a list of all the namespaces the Cluster Operator will watch to the STRIMZI_NAMESPACE environment variable.

For example, in this procedure the Cluster Operator will watch the namespaces watched-namespace-1, watched-namespace-2, watched-namespace-3.

apiVersion: apps/v1
kind: Deployment
spec:
  # ...
  template:
    spec:
      serviceAccountName: strimzi-cluster-operator
      containers:
      - name: strimzi-cluster-operator
        image: quay.io/strimzi/operator:0.43.0
        imagePullPolicy: IfNotPresent
        env:
        - name: STRIMZI_NAMESPACE
          value: watched-namespace-1,watched-namespace-2,watched-namespace-3
For each namespace listed, install the RoleBindings.

In this example, we replace watched-namespace in these commands with the namespaces listed in the previous step, repeating them for watched-namespace-1, watched-namespace-2, watched-namespace-3:

kubectl create -f install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml -n <watched_namespace>
kubectl create -f install/cluster-operator/023-RoleBinding-strimzi-cluster-operator.yaml -n <watched_namespace>
kubectl create -f install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml -n <watched_namespace>
Deploy the Cluster Operator:

kubectl create -f install/cluster-operator -n my-cluster-operator-namespace
Check the status of the deployment:

kubectl get deployments -n my-cluster-operator-namespace
Output shows the deployment name and readiness
NAME                      READY  UP-TO-DATE  AVAILABLE
strimzi-cluster-operator  1/1    1           1
READY shows the number of replicas that are ready/expected. The deployment is successful when the AVAILABLE output shows 1.

6.2.4. Deploying the Cluster Operator to watch all namespaces
This procedure shows how to deploy the Cluster Operator to watch Strimzi resources across all namespaces in your Kubernetes cluster.

When running in this mode, the Cluster Operator automatically manages clusters in any new namespaces that are created.

Prerequisites
You need an account with permission to create and manage CustomResourceDefinition and RBAC (ClusterRole, and RoleBinding) resources.

Procedure
Edit the Strimzi installation files to use the namespace the Cluster Operator is going to be installed into.

For example, in this procedure the Cluster Operator is installed into the namespace my-cluster-operator-namespace.

On Linux, use:

sed -i 's/namespace: .*/namespace: my-cluster-operator-namespace/' install/cluster-operator/*RoleBinding*.yaml
On MacOS, use:

sed -i '' 's/namespace: .*/namespace: my-cluster-operator-namespace/' install/cluster-operator/*RoleBinding*.yaml
Edit the install/cluster-operator/060-Deployment-strimzi-cluster-operator.yaml file to set the value of the STRIMZI_NAMESPACE environment variable to *.

apiVersion: apps/v1
kind: Deployment
spec:
  # ...
  template:
    spec:
      # ...
      serviceAccountName: strimzi-cluster-operator
      containers:
      - name: strimzi-cluster-operator
        image: quay.io/strimzi/operator:0.43.0
        imagePullPolicy: IfNotPresent
        env:
        - name: STRIMZI_NAMESPACE
          value: "*"
        # ...
Create ClusterRoleBindings that grant cluster-wide access for all namespaces to the Cluster Operator.

kubectl create clusterrolebinding strimzi-cluster-operator-namespaced --clusterrole=strimzi-cluster-operator-namespaced --serviceaccount my-cluster-operator-namespace:strimzi-cluster-operator
kubectl create clusterrolebinding strimzi-cluster-operator-watched --clusterrole=strimzi-cluster-operator-watched --serviceaccount my-cluster-operator-namespace:strimzi-cluster-operator
kubectl create clusterrolebinding strimzi-cluster-operator-entity-operator-delegation --clusterrole=strimzi-entity-operator --serviceaccount my-cluster-operator-namespace:strimzi-cluster-operator
Deploy the Cluster Operator to your Kubernetes cluster.

kubectl create -f install/cluster-operator -n my-cluster-operator-namespace
Check the status of the deployment:

kubectl get deployments -n my-cluster-operator-namespace
Output shows the deployment name and readiness
NAME                      READY  UP-TO-DATE  AVAILABLE
strimzi-cluster-operator  1/1    1           1
READY shows the number of replicas that are ready/expected. The deployment is successful when the AVAILABLE output shows 1.

6.3. Deploying Kafka
To be able to manage a Kafka cluster with the Cluster Operator, you must deploy it as a Kafka resource. Strimzi provides example deployment files to do this. You can use these files to deploy the Topic Operator and User Operator at the same time.

After you have deployed the Cluster Operator, use a Kafka resource to deploy the following components:

A Kafka cluster that uses KRaft or ZooKeeper:

KRaft-based Kafka cluster

ZooKeeper-based Kafka cluster

Topic Operator

User Operator

Node pools are used in the deployment of a Kafka cluster in KRaft (Kafka Raft metadata) mode, and may be used for the deployment of a Kafka cluster with ZooKeeper. Node pools represent a distinct group of Kafka nodes within the Kafka cluster that share the same configuration. For each Kafka node in the node pool, any configuration not defined in node pool is inherited from the cluster configuration in the Kafka resource.

If you haven’t deployed a Kafka cluster as a Kafka resource, you can’t use the Cluster Operator to manage it. This applies, for example, to a Kafka cluster running outside of Kubernetes. However, you can use the Topic Operator and User Operator with a Kafka cluster that is not managed by Strimzi, by deploying them as standalone components. You can also deploy and use other Kafka components with a Kafka cluster not managed by Strimzi.

6.3.1. Deploying a Kafka cluster in KRaft mode
This procedure shows how to deploy a Kafka cluster in KRaft mode and associated node pools using the Cluster Operator.

The deployment uses a YAML file to provide the specification to create a Kafka resource and KafkaNodePool resources.

Strimzi provides the following example deployment files that you can use to create a Kafka cluster that uses node pools:

kafka/kraft/kafka-with-dual-role-nodes.yaml
Deploys a Kafka cluster with one pool of nodes that share the broker and controller roles.

kafka/kraft/kafka.yaml
Deploys a persistent Kafka cluster with one pool of controller nodes and one pool of broker nodes.

kafka/kraft/kafka-ephemeral.yaml
Deploys an ephemeral Kafka cluster with one pool of controller nodes and one pool of broker nodes.

kafka/kraft/kafka-single-node.yaml
Deploys a Kafka cluster with a single node.

kafka/kraft/kafka-jbod.yaml
Deploys a Kafka cluster with multiple volumes in each broker node.

In this procedure, we use the example deployment file that deploys a Kafka cluster with one pool of nodes that share the broker and controller roles.

The Kafka resource configuration for each example includes the strimzi.io/node-pools: enabled annotation, which is required when using node pools. Kafka resources using KRaft mode must also have the annotation strimzi.io/kraft: enabled.

The example YAML files specify the latest supported Kafka version and KRaft metadata version used by the Kafka cluster.

Note
You can perform the steps outlined here to deploy a new Kafka cluster with KafkaNodePool resources or migrate your existing Kafka cluster.
Prerequisites
The Cluster Operator must be deployed.

Before you begin
By default, the example deployment files specify my-cluster as the Kafka cluster name. The name cannot be changed after the cluster has been deployed. To change the cluster name before you deploy the cluster, edit the Kafka.metadata.name property of the Kafka resource in the relevant YAML file.

Procedure
Deploy a KRaft-based Kafka cluster.

To deploy a Kafka cluster with a single node pool that uses dual-role nodes:

kubectl apply -f examples/kafka/kraft/kafka-with-dual-role-nodes.yaml
Check the status of the deployment:

kubectl get pods -n <my_cluster_operator_namespace>
Output shows the node pool names and readiness
NAME                        READY  STATUS   RESTARTS
my-cluster-entity-operator  3/3    Running  0
my-cluster-pool-a-0         1/1    Running  0
my-cluster-pool-a-1         1/1    Running  0
my-cluster-pool-a-4         1/1    Running  0
my-cluster is the name of the Kafka cluster.

pool-a is the name of the node pool.

A sequential index number starting with 0 identifies each Kafka pod created. If you are using ZooKeeper, you’ll also see the ZooKeeper pods.

READY shows the number of replicas that are ready/expected. The deployment is successful when the STATUS displays as Running.

Information on the deployment is also shown in the status of the KafkaNodePool resource, including a list of IDs for nodes in the pool.

Note
Node IDs are assigned sequentially starting at 0 (zero) across all node pools within a cluster. This means that node IDs might not run sequentially within a specific node pool. If there are gaps in the sequence of node IDs across the cluster, the next node to be added is assigned an ID that fills the gap. When scaling down, the node with the highest node ID within a pool is removed.